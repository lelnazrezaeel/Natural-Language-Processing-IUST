{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWng3dudb_R"
      },
      "source": [
        "# Colab and Numpy\n",
        "\n",
        "## 1. Softmax\n",
        "If you don't remember Softmax details, you can visit here:\n",
        "https://en.wikipedia.org/wiki/Softmax_function \\\\ \\\\\n",
        "\n",
        "Write a function that computes the softmax using numpy functions. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve2vNVvleSIJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx3GVD-VdP4H"
      },
      "outputs": [],
      "source": [
        "def Softmax(logits):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPZdKMfSea7S"
      },
      "source": [
        "Let's say the logits for the output layer of your neural network are the last 4 digits of your student id. Run your code above and output the softmax values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFcbzJSkeqt0"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g43HWa65e2S8"
      },
      "source": [
        "##2. Temperature\n",
        "\n",
        "Write a new function to calculate softmax with a Temperature parameter. \\\\\n",
        "\n",
        "\n",
        "![images.jpg](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM1lTkBUeu5U"
      },
      "outputs": [],
      "source": [
        "def Softmax(logits, T = 1):\n",
        "    pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "011G-I7mfxxW"
      },
      "source": [
        "### Temperature Values\n",
        "\n",
        "Provide the softmax value from the logits above for various temperatures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc9doVC4gIOJ"
      },
      "source": [
        "#### T = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0WxzAjqfxWK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMne1YBxgJ4_"
      },
      "source": [
        "#### T = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vU3OedUgNGm"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyZYKw_MgNco"
      },
      "source": [
        "#### T = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKqaK7v5gQ-x"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH-EwwRQgSWI"
      },
      "source": [
        "## 3. Conclusion\n",
        "\n",
        "Explain how temperature affects logits and what it's good for?\n",
        "\n",
        "\\# YOUR EXPLANATION HERE \\#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c68wAexvhy_P"
      },
      "source": [
        "# Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFPWF8DSvee4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dh9cs9-h1dm",
        "outputId": "039e9c10-b9f1-4d4d-899c-45f047dff324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "# install transformers library\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-D2fzOh49E"
      },
      "source": [
        "Load GPT2-Large from https://huggingface.co/gpt2-large \\\\\n",
        "Do not forget to load with it's LM head for generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhK1l52kPvs"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "tokenizer = None\n",
        "model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyA2N1O4oenD"
      },
      "source": [
        "## 1. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrtD__B7oiz0"
      },
      "source": [
        "Language model heads at huggingface, provide us auto-regressive text generation their respective GenerationMixin class. \\\\ \n",
        "\n",
        "First of all, take a look at [generate](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) function and its arguments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEJrNs9urVxA"
      },
      "source": [
        "### 1.1 Greedy decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1LTxnp2sYH7"
      },
      "source": [
        "By default, this function generates with greedy decoding. To get started, please resume this text with greedy strategy with **maximum sequence length of 50** \\\\\n",
        "You can simply call model.generate(**args), all you have to do is to figure out the right arguments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9cYkBuBlVSD"
      },
      "outputs": [],
      "source": [
        "text = \"There is an exam on Thursday morning and\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkhHcA4trcmi"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ###\n",
        "greedy_output = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux7yBki3l3QE",
        "outputId": "e747c9fc-2172-40a0-fb5c-658efe772b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There is an exam on Thursday morning and I will be there. I will be there to answer questions and to answer questions. I will be there to answer questions and to answer questions. I will be there to answer questions and to answer questions. I\n"
          ]
        }
      ],
      "source": [
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agFDBJ3PtYxV"
      },
      "source": [
        "### 1.2 Beam Search\n",
        "Take a look at [generate](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) function and add a new argument to use beam search strategy. **Beams size is 5. Limit maximum length to 50.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhm3UL-xmm4D",
        "outputId": "aa36069e-5460-4ffd-b08f-21d2bfe33461"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE ###\n",
        "beam_output = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgvRE-zqnIFa",
        "outputId": "b17ca942-862a-4632-a8db-6bb9b13ef2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There is an exam on Thursday morning and I'm not sure if I'm going to be able to pass it,\" he said.\n",
            "\n",
            "\"I'm not sure if I'm going to be able to pass it. I'm not sure if I\n"
          ]
        }
      ],
      "source": [
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybiQ-A_Lt6uQ"
      },
      "source": [
        "### 1.3 STOP REPEATING!\n",
        "As you can see, there are repeating ngrams! Let's make our generation a bit cleaneer. **Again, using beam size of 5, try not repeating ngrams of size 2. Limit maximum length to 50.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCgRyb1SnQYJ",
        "outputId": "e1983045-ae4c-42ed-faa2-e63fbe218bc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "beam_output = None "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgXD_4r6njvk",
        "outputId": "765ffdff-55dc-45f6-e1e1-b7e148f7e8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There is an exam on Thursday morning and I have to take it,\" he said.\n",
            "\n",
            "\"I am not sure if I will be able to do it, but I am going to try my best.\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTQVB-uyv190"
      },
      "source": [
        "### 1.4 Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRCFSmWP13tX"
      },
      "source": [
        "Compare these three outputs and explain how we can make it better.\n",
        "\n",
        "######################\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwh_-GSju6rP"
      },
      "source": [
        "## 2. Push to hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkA-M9nQu9KT"
      },
      "source": [
        "Push your GPT2-Large to hub. Remember you have to be a member of our organization, or else we are unable to locate your model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9jRlOj6u8PX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE #"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tduMlyXgvuwy"
      },
      "source": [
        "# Temperature at generation (extra point) \n",
        "\n",
        "Can you explain temperature argument in generation? How to use it? Provide us a config that model have multiple choices with nearly same probability.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAYJzthGvxse"
      },
      "outputs": [],
      "source": [
        "# do_sample = True\n",
        "# temperature = 100\n",
        "# top_k  = 40 # or top_p = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ZP2yR1zo1M"
      },
      "source": [
        "What happens if T &#8594; 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM4sjO-60K1w"
      },
      "source": [
        "##########\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tzWng3dudb_R",
        "g43HWa65e2S8",
        "011G-I7mfxxW",
        "FEJrNs9urVxA",
        "agFDBJ3PtYxV",
        "ybiQ-A_Lt6uQ",
        "Nwh_-GSju6rP"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
