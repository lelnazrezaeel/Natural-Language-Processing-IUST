{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QITBI9Ckkf-"
      },
      "source": [
        "First find the embeddings of all words in the dataset and then pick a random word and find 10 words that are close to it.Your metric to find similarity should be **Euclidean Distance**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNFhnSCkn9kg"
      },
      "source": [
        "#Load Pretrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7gO5NaikSPy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxAUKzFLqca-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5N_om_cl5H2"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"bert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkhOhpT_mBt8"
      },
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained(model_checkpoint, output_hidden_states = True)\n",
        "model.eval()\n",
        "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CySos-hmSjb"
      },
      "outputs": [],
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "  \"\"\"\n",
        "  Preprocesses text input in a way that BERT can interpret.\n",
        "  \"\"\"\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1]*len(indexed_tokens)\n",
        "  # convert inputs to tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensor = torch.tensor([segments_ids])\n",
        "  return tokenized_text, tokens_tensor, segments_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0MPeZyzmU16"
      },
      "outputs": [],
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
        "  \"\"\"\n",
        "  Obtains BERT embeddings for tokens.\n",
        "  \"\"\"\n",
        "  # gradient calculation id disabled\n",
        "  with torch.no_grad():\n",
        "    # obtain hidden states\n",
        "    outputs = model(tokens_tensor, segments_tensor)\n",
        "    hidden_states = outputs[2]\n",
        "  # concatenate the tensors for all layers\n",
        "  # use \"stack\" to create new dimension in tensor\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  # remove dimension 1, the \"batches\"\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  # swap dimensions 0 and 1 so we can loop over tokens\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  # intialized list to store embeddings\n",
        "  token_vecs = []\n",
        "  # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
        "  # where Y is the number of tokens in the sentence\n",
        "  # loop over tokens in sentence\n",
        "  for token in token_embeddings:\n",
        "  # \"token\" is a [12 x 768] tensor\n",
        "  # sum the vectors from the last four layers\n",
        "      token_vec = token[-1]\n",
        "      token_vecs.append(token_vec)\n",
        "  return token_vecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJQqZXQ1n1BB"
      },
      "source": [
        "#Load Dataset and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr4fwITmnc9o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"q1_sent_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRQ1HPCXm7QA"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_gwXR07swHQ"
      },
      "outputs": [],
      "source": [
        "import string \n",
        "special_tokens = ['[UNK]', '[CLS]', '[SEP]']\n",
        "def remove_unuseful_tokens(tokens):\n",
        "  tokens_without_stopwords = list()\n",
        "  for token in context_sorted:\n",
        "    if token in string.punctuation:\n",
        "      continue\n",
        "    elif '#' in token:\n",
        "      continue\n",
        "    elif token in special_tokens:\n",
        "      continue\n",
        "    elif token[0] == '[':\n",
        "      continue\n",
        "    else:\n",
        "      tokens_without_stopwords.append(token)\n",
        "  return tokens_without_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkvxlIyonNkr"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from files and save them in memory base on each year.\n",
        "\n",
        "texts = df[\"text\"]\n",
        "normalized_list = []\n",
        "for text in texts:\n",
        "  new_text = delete_url(text)\n",
        "  new_text = delete_hashtag_usernames(new_text)\n",
        "  normalized_list.append(new_text)\n",
        "\n",
        "normalized_texts = pd.DataFrame(normalized_list, columns=['text'])\n",
        "tweets = normalized_texts[\"text\"]\n",
        "\n",
        "# 2. remove urls, hashtags and usernames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEkdyxuCmkmw"
      },
      "outputs": [],
      "source": [
        "sentences = tweets[:1000]\n",
        "from collections import OrderedDict\n",
        "context_dict = {} # key is token and values are (embeddings, count)\n",
        "result_dict1 = dict()\n",
        "context_embeddings = []\n",
        "context_tokens = []\n",
        "for sentence in sentences:\n",
        "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  # make ordered dictionary to keep track of the position of each   word\n",
        "  tokens = OrderedDict()\n",
        "  # loop over tokens in sensitive sentence\n",
        "  for token in tokenized_text[1:-1]:\n",
        "    # keep track of position of word and whether it occurs multiple times\n",
        "    if token in tokens:\n",
        "      tokens[token] += 1\n",
        "    else:\n",
        "      tokens[token] = 1\n",
        "    # compute the position of the current token\n",
        "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
        "    current_index = token_indices[tokens[token]-1]\n",
        "    # get the corresponding embedding\n",
        "    token_vec = list_token_embeddings[current_index]\n",
        "    \n",
        "    # save values\n",
        "    if token in context_dict:\n",
        "      context_dict[token]['embedding'] += token_vec\n",
        "      context_dict[token]['count'] += 1\n",
        "    else:\n",
        "      context_dict[token] = {'embedding': token_vec, 'count': 1}\n",
        "\n",
        "context_sorted = sorted(context_dict, key=lambda k: context_dict[k]['count'])\n",
        "context_sorted.reverse()\n",
        "\n",
        "tokens_without_stopwords = remove_unuseful_tokens(context_sorted)\n",
        "\n",
        "for token in tokens_without_stopwords[:1000]:\n",
        "  context_tokens.append(token)\n",
        "  context_embeddings.append(context_dict[token]['embedding'] / context_dict[token]['count'])\n",
        "  result_dict1[token] = context_dict[token]['embedding'] / context_dict[token]['count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoRWJgzCoGVb"
      },
      "source": [
        "#Find 10 Nearest neighbor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md8pCSKVnpOu"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  # Impelement Euclidean Distance and them find k nearest neighbors of word using this metric\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO80I89ZmyFb"
      },
      "outputs": [],
      "source": [
        "word = '' # Pick a random word from dataset\n",
        "# 2. find 10 nearest words\n",
        "top_10_nearest_words = find_k_nearest_neighbors(word, result_dict1, 10)\n",
        "\n",
        "# Print the top-10 words\n",
        "for i, word in enumerate(top_10_nearest_words):\n",
        "    print(f\"{i}- {word}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cNFhnSCkn9kg",
        "YoRWJgzCoGVb"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
