{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-12T14:41:02.111260Z","iopub.execute_input":"2023-05-12T14:41:02.111563Z","iopub.status.idle":"2023-05-12T14:41:02.125438Z","shell.execute_reply.started":"2023-05-12T14:41:02.111537Z","shell.execute_reply":"2023-05-12T14:41:02.124319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-05-12T14:41:02.127517Z","iopub.execute_input":"2023-05-12T14:41:02.128261Z","iopub.status.idle":"2023-05-12T14:41:02.134723Z","shell.execute_reply.started":"2023-05-12T14:41:02.128189Z","shell.execute_reply":"2023-05-12T14:41:02.133805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\nbert = BertModel.from_pretrained(\"bert-base-uncased\")\nembedding_matrix = bert.embeddings.word_embeddings.weight","metadata":{"execution":{"iopub.status.busy":"2023-05-12T14:42:15.856941Z","iopub.execute_input":"2023-05-12T14:42:15.857631Z","iopub.status.idle":"2023-05-12T14:42:35.469035Z","shell.execute_reply.started":"2023-05-12T14:42:15.857586Z","shell.execute_reply":"2023-05-12T14:42:35.468074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAD_ID = 0\nCLS_ID = 101\ndevice = \"cuda:0\"","metadata":{"execution":{"iopub.status.busy":"2023-05-12T14:42:35.471117Z","iopub.execute_input":"2023-05-12T14:42:35.471490Z","iopub.status.idle":"2023-05-12T14:42:35.478423Z","shell.execute_reply.started":"2023-05-12T14:42:35.471454Z","shell.execute_reply":"2023-05-12T14:42:35.475665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Here is some text to encode\"\ninput_ids = tokenizer.encode(input_text, add_special_tokens=True)\n# you can get BERT embeddings like this:\nembedding_matrix[input_ids].shape, input_ids","metadata":{"execution":{"iopub.status.busy":"2023-05-12T14:42:42.857409Z","iopub.execute_input":"2023-05-12T14:42:42.857771Z","iopub.status.idle":"2023-05-12T14:42:42.880825Z","shell.execute_reply.started":"2023-05-12T14:42:42.857727Z","shell.execute_reply":"2023-05-12T14:42:42.879826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's begin !\nfrom torch.utils.data import Dataset\nimport pickle\nimport json\nimport csv \nimport torch\n\nclass VQADataset(Dataset):\n\n    def __init__(self, split_path):\n        image_features_path = \"/kaggle/input/iust-vqa/image_features.pickle\"\n        answers_list_path = \"/kaggle/input/iust-vqa/answer_list.txt\"\n        image2questions_path = \"/kaggle/input/iust-vqa/image_question.json\"\n        \n        ## Read image features, use pickle!\n        with open(image_features_path, 'rb') as f:\n            ### YOUR CODE HERE\n            self.image_features = pickle.load(f)\n            ### YOUR CODE HERE\n        \n        ##sample: self.question2img[q_id] = img_id\n        self.question2img = {}\n        \n        ##sample: self.questions[q_id] = {\"text\" : q_text, \"tokenized\" : tokenized_question}\n        ## tokenization: tokenizer.encode(sentence)\n        self.questions = {}\n        \n        with open(image2questions_path, 'r') as f:\n            ## YOUR CODE HERE\n            ## Load json file (image2questions)\n            data = json.load(f)\n            \n            ## retrieve requested values \"self.question2img\", \"self.questions\" from givenn json\n            ## ~ 6 lines\n            for img_id in data:\n                for question in data[img_id]:\n                    q_id = question[0]\n                    q_text = question[1]\n                    \n                    self.questions[q_id] = {\"text\" : q_text, \"tokenized\" : tokenizer.encode(q_text)}\n                    self.question2img[q_id] = img_id\n            ### YOUR CODE HERE\n        \n        self.possible_answers = []\n        with open(answers_list_path, 'r') as f:\n            ## read answers list from text file, save them in an array\n            self.possible_answers = f.read().split()\n        \n        ## sample: self.data[idx] = q_id\n        self.data = []\n        ## sample: self.labels[idx] = 4\n        self.labels = []\n        \n        \n        \n        ## load data from \"split_path\", fill self.data and self.labels as requested! take a look at train.csv\n        # https://docs.python.org/3/library/csv.html#csv.DictReader\n        with open(split_path, newline='') as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                self.data.append(int(row['question_id']))\n                if (row['label'] is not None):\n                    self.labels.append(int(row['label']))\n                else:\n                    self.labels = None\n\n    def __getitem__(self, idx):\n        \"\"\"This method returns tuple of (question_id, image_features (Tensor), tokenized_question (Tensor), label\n        \n        Note: label can be None!\n        \"\"\"\n        \n        \n        q_id = self.data[idx]\n        ### YOUR CODE HERE\n        ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n        #torch.tensor([0]).to(device)  *GPU\n        q_tokenized = self.questions[q_id]['tokenized']\n        img_id =self.question2img[q_id]\n        label = None\n        if(self.labels is not None):\n            label = self.labels[idx]\n        return q_id, torch.tensor(self.image_features[img_id]).to(device), torch.tensor(q_tokenized).to(device), label\n        ### YOUR CODE HERE\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T16:12:43.434466Z","iopub.execute_input":"2023-05-12T16:12:43.434841Z","iopub.status.idle":"2023-05-12T16:12:43.448708Z","shell.execute_reply.started":"2023-05-12T16:12:43.434812Z","shell.execute_reply":"2023-05-12T16:12:43.447792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n    \"\"\"\n        Batch post processing, we can pad questions! \n        returns q_ids, images (Tensor), questions(Tensor), labels (Tensor)\n    \"\"\"\n    images = []\n    questions = []\n    labels = []\n    q_ids = []\n    \n    ### YOUR CODE HERE\n    ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n    for q_id, img, q_tokens, label in batch:\n        q_ids.append(q_id)\n        images.append(img)\n        questions.append(q_tokens)\n        if (label is not None):\n            labels.append(torch.tensor(label))\n        else:\n            labels = None\n    ### Stack images into one tensor\n    ## torch.stack, shape must be (batch_size, img_features)\n    images = torch.stack(images, dim = 0)\n    \n    ## stack labels if they're not None, else make labels None!\n    if (labels is not None):\n        labels = torch.stack(labels, dim = 0)\n    else:\n        labels = None\n    \n    ## pad questions, shape must be (batch_size, longest_sentence)\n    ## https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n    questions = pad_sequence(questions, padding_value = 0, batch_first = True)\n    \n    \n    return q_ids, images, questions, labels","metadata":{"execution":{"iopub.status.busy":"2023-05-12T16:39:53.284889Z","iopub.execute_input":"2023-05-12T16:39:53.285888Z","iopub.status.idle":"2023-05-12T16:39:53.296071Z","shell.execute_reply.started":"2023-05-12T16:39:53.285850Z","shell.execute_reply":"2023-05-12T16:39:53.295092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndset = VQADataset(\"/kaggle/input/iust-vqa/train.csv\")\ndata_loader_train = DataLoader(dset, collate_fn=collate_batch, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T16:39:54.723474Z","iopub.execute_input":"2023-05-12T16:39:54.723833Z","iopub.status.idle":"2023-05-12T16:39:54.809150Z","shell.execute_reply.started":"2023-05-12T16:39:54.723805Z","shell.execute_reply":"2023-05-12T16:39:54.808266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n## Nothing, just look =)))\n\nclass PositionalEncoder(nn.Module):\n    \"\"\"Positional encoding class pulled from the PyTorch documentation tutorial\n    on Transformers for seq2seq models:\n    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoder, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()\\\n                             * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T16:39:56.508248Z","iopub.execute_input":"2023-05-12T16:39:56.508611Z","iopub.status.idle":"2023-05-12T16:39:56.520896Z","shell.execute_reply.started":"2023-05-12T16:39:56.508581Z","shell.execute_reply":"2023-05-12T16:39:56.520003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport math\n\n#The most interesting part!\n\nclass VQA_Simple(nn.Module):\n    def __init__(self, dropout, text_hidden_size, n_layers, n_heads, image_hidden_size, n_outputs):\n        super().__init__()\n        self.dropout = dropout\n        self.d_model = text_hidden_size\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.image_hidden_size = image_hidden_size\n        self.PAD = PAD_ID\n        \n        self.embedding_matrix = bert.embeddings.word_embeddings.weight\n        \n        \n        ##initilize TransformerEncoderLayer\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n        encoder_layer = nn.TransformerEncoderLayer(d_model = self.d_model,\n                                                   nhead = self.n_heads,\n                                                   dropout = self.dropout)\n        \n        ##initilize TransformerEncoder\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n        self.t_encoder = nn.TransformerEncoder(encoder_layer, num_layers = self.n_layers)\n        \n        ##if you looke enough, you can initilize positional encoder!!\n        self.pe = PositionalEncoder(d_model = self.d_model, dropout = self.dropout)\n        \n        ##initilize TransformerDecoderLayer\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html\n        decoder_layer = nn.TransformerDecoderLayer(d_model = self.d_model,\n                                                   nhead = self.n_heads)\n        \n        ##initilize TransformerDecoder\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n        self.t_decoder = nn.TransformerDecoder(decoder_layer, num_layers = self.n_layers)\n        \n        ##Linear output, recieves concatenation of text and image features, outputs final answer!\n        self.linear = nn.Linear(self.d_model + self.image_hidden_size, n_outputs)\n        \n    def forward(self, images, input_ids):\n        ##images shape: (batch_size, img_features)\n        ##input_ids shape: (batch_size, sequence_len)\n        b_size = images.shape[0]\n        \n        ### YOUR CODE HERE\n        \n        ## Calculate masks, shape: (batch_size, sequence_len)\n        src_key_mask = (input_ids == self.PAD)\n        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n        ##shape should be (sequence_len, batch_size, text_embedding_features)\n        embeddings = self.embedding_matrix[input_ids].permute(1, 0, 2)\n        \n        ##Positional embeddings\n        ##shape should be (sequence_len, batch_size, text_embedding_features)\n        positional_embeddings = self.pe(embeddings)\n        \n        ## feed positinal_embeddings to the encoder!\n        ## output shape should be (sequence_len, batch_size, d_model)\n        ## additional args:  src_key_padding_mask\n        encoder_output = self.t_encoder(positional_embeddings, src_key_padding_mask=src_key_mask)\n        \n        ##(batch_size, 1)\n        tgt = torch.tensor([CLS_ID] * b_size).unsqueeze(1).to(device)\n        ##(batch_size, 1)\n        tgt_key_padding_mask = (tgt == self.PAD)\n        \n        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n        ##shape should be (1, batch_size, text_embedding_features)\n        tgt_embeddings = self.embedding_matrix[tgt].permute(1, 0, 2)\n        \n\n        # target attention masks to avoid future tokens in our predictions\n        # Adapted from PyTorch source code:\n        # https://github.com/pytorch/pytorch/blob/176174a68ba2d36b9a5aaef0943421682ecc66d4/torch/nn/modules/transformer.py#L130\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(1).to(device)\n        \n        ## Positional embedding \n        tgt_positions = self.pe(tgt_embeddings)\n        \n        output = self.t_decoder(tgt=tgt_positions, \n                                memory=encoder_output,\n                                tgt_mask=tgt_mask,\n                                tgt_key_padding_mask = tgt_key_padding_mask, \n                                memory_key_padding_mask = src_key_mask) ##(1, batch_size, text_embedding_features)\n        \n        \n        \n        output_text = output.permute(1, 0, 2).squeeze(1) ## (batch_size, text_embedding_features)\n        \n        ##https://pytorch.org/docs/stable/generated/torch.cat.html\n        #concatenate text output and image features\n        concatenated = torch.cat([images, output_text], dim=1).to(device)\n        \n        \n        y = self.linear(concatenated)\n        \n        return y\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:10:28.189295Z","iopub.execute_input":"2023-05-12T17:10:28.189711Z","iopub.status.idle":"2023-05-12T17:10:28.205737Z","shell.execute_reply.started":"2023-05-12T17:10:28.189679Z","shell.execute_reply":"2023-05-12T17:10:28.204529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmodel = VQA_Simple(dropout=0.1, \n                   text_hidden_size=768, \n                   n_layers=2, \n                   n_heads=6, \n                   image_hidden_size=512, \n                   n_outputs=10).cuda()\nlr=1e-4\nepochs = 25\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(list(model.parameters()), lr=lr)\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (q_ids, images, questions, labels) in enumerate(pbar := tqdm(data_loader_train, total=len(data_loader_train))):\n        pbar.set_description(f\"Epoch {epoch}\")\n        \n        optimizer.zero_grad()\n        output = model(images, questions)\n        \n        loss = criterion(output.cpu(), labels)\n        running_loss += loss\n        \n        loss.backward()\n        optimizer.step()\n        log_interval = 5\n        pbar.set_postfix(loss=running_loss/(i+1))\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:14:22.022926Z","iopub.execute_input":"2023-05-12T17:14:22.023347Z","iopub.status.idle":"2023-05-12T17:14:49.086942Z","shell.execute_reply.started":"2023-05-12T17:14:22.023315Z","shell.execute_reply":"2023-05-12T17:14:49.085953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(data_loader, net):\n    predicts = []\n    ids = []\n    net.eval()\n    for i, (q_ids, images, questions, _) in enumerate(pbar := tqdm(data_loader, total=len(data_loader))):\n        outputs = net(images, questions)\n        outputs = torch.argmax(outputs, dim=1)\n        predicts.extend(outputs.cpu().tolist())\n        ids.extend(q_ids)\n    return predicts, ids\n\n\ntest_dset = VQADataset(\"/kaggle/input/iust-vqa/test.csv\")\ndata_loader_test = DataLoader(test_dset, collate_fn=collate_batch, batch_size=8)\npreds, ids = predict(data_loader_test, model)\n\n# with open(\"output.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:15:26.376749Z","iopub.execute_input":"2023-05-12T17:15:26.377125Z","iopub.status.idle":"2023-05-12T17:15:26.588741Z","shell.execute_reply.started":"2023-05-12T17:15:26.377094Z","shell.execute_reply":"2023-05-12T17:15:26.587769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\noutput_data = {\"question_id\": [str(id) for id in ids], \"label\": preds}\ndf = pd.DataFrame(output_data)\ndf.to_csv(\"/kaggle/working/output.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:15:33.083821Z","iopub.execute_input":"2023-05-12T17:15:33.084250Z","iopub.status.idle":"2023-05-12T17:15:33.108634Z","shell.execute_reply.started":"2023-05-12T17:15:33.084215Z","shell.execute_reply":"2023-05-12T17:15:33.107585Z"},"trusted":true},"execution_count":null,"outputs":[]}]}