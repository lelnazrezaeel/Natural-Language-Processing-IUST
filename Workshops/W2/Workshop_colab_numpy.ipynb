{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "amvA0I3_qCGm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome There!\n",
        "Let us begin! \n",
        "\n",
        "The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code."
      ],
      "metadata": {
        "id": "Lk6vS0DPqj4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ],
      "metadata": {
        "id": "Kb9jHQCjtdQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zOdu_7seCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572624cc-28a8-4629-8e96-b7b6e0051869"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n",
        "\n",
        "Variables that you define in one cell can later be used in other cells:"
      ],
      "metadata": {
        "id": "mLThMGqBupRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iSOMgcBuqRY",
        "outputId": "3d6364d4-dbe8-4699-ed2b-bd71546a7150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###System Level Commands\n",
        "\n",
        "You can run any (allowed) system commands usin \"!\" sign."
      ],
      "metadata": {
        "id": "kSwSodXY6KRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_V1RlZ69Vu",
        "outputId": "27188d40-8609-45cc-e5a4-e085576025fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t dev   lib32   mnt\t\t\t root  sys    var\n",
            "boot\t etc   lib64   NGC-DL-CONTAINER-LICENSE  run   tmp\n",
            "content  home  libx32  opt\t\t\t sbin  tools\n",
            "datalab  lib   media   proc\t\t\t srv   usr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6YaQv6r7KsR",
        "outputId": "df165eca-7d4c-4fb9-d448-13c487eb802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n",
            "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n",
            "           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n",
            "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n",
            "           <command> [<args>]\n",
            "\n",
            "These are common Git commands used in various situations:\n",
            "\n",
            "start a working area (see also: git help tutorial)\n",
            "   clone             Clone a repository into a new directory\n",
            "   init              Create an empty Git repository or reinitialize an existing one\n",
            "\n",
            "work on the current change (see also: git help everyday)\n",
            "   add               Add file contents to the index\n",
            "   mv                Move or rename a file, a directory, or a symlink\n",
            "   restore           Restore working tree files\n",
            "   rm                Remove files from the working tree and from the index\n",
            "   sparse-checkout   Initialize and modify the sparse-checkout\n",
            "\n",
            "examine the history and state (see also: git help revisions)\n",
            "   bisect            Use binary search to find the commit that introduced a bug\n",
            "   diff              Show changes between commits, commit and working tree, etc\n",
            "   grep              Print lines matching a pattern\n",
            "   log               Show commit logs\n",
            "   show              Show various types of objects\n",
            "   status            Show the working tree status\n",
            "\n",
            "grow, mark and tweak your common history\n",
            "   branch            List, create, or delete branches\n",
            "   commit            Record changes to the repository\n",
            "   merge             Join two or more development histories together\n",
            "   rebase            Reapply commits on top of another base tip\n",
            "   reset             Reset current HEAD to the specified state\n",
            "   switch            Switch branches\n",
            "   tag               Create, list, delete or verify a tag object signed with GPG\n",
            "\n",
            "collaborate (see also: git help workflows)\n",
            "   fetch             Download objects and refs from another repository\n",
            "   pull              Fetch from and integrate with another repository or a local branch\n",
            "   push              Update remote refs along with associated objects\n",
            "\n",
            "'git help -a' and 'git help -g' list available subcommands and some\n",
            "concept guides. See 'git help <command>' or 'git help <concept>'\n",
            "to read about a specific subcommand or concept.\n",
            "See 'git help git' for an overview of the system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pfhvL0j7zYZ",
        "outputId": "496fe229-f64e-4ffb-f4f3-ae47e5dd95e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"20.04.5 LTS (Focal Fossa)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 20.04.5 LTS\"\n",
            "VERSION_ID=\"20.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=focal\n",
            "UBUNTU_CODENAME=focal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning \n",
        "Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).\n",
        "\n",
        "Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org).\n",
        "\n",
        "Colab is used extensively in the machine learning community with applications including:\n",
        "- Getting started with Torch, TensorFlow\n",
        "- Developing and training neural networks\n",
        "- Experimenting with TPUs\n",
        "- Disseminating AI research\n",
        "- Creating tutorials"
      ],
      "metadata": {
        "id": "OzWr5IVBu1ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Working with Data\n",
        "We mostly use CSV, json, and sometimes pickles to store our data. You can save them either in your local system or google drive and simply use them in colab. \n"
      ],
      "metadata": {
        "id": "N2UwsAFailYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mounting Google Drive locally\n",
        "The example below shows how to mount your Google Drive on your runtime using an authorization code, and how to write and read files there. Once executed, you will be able to see the new file (`foo.txt`) at [https://drive.google.com/](https://drive.google.com/).\n",
        "\n",
        "This only supports reading, writing, and moving files; to programmatically modify sharing settings or other metadata, use one of the other options below.\n",
        "\n",
        "**Note:** When using the 'Mount Drive' button in the file browser, no authentication codes are necessary for notebooks that have only been edited by the current user."
      ],
      "metadata": {
        "id": "VulrYatQjHhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zJoKKxM4u0zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/drive/My\\ Drive/foo.txt"
      ],
      "metadata": {
        "id": "Up9PG8FNjSqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c28c705-0d62-4193-efa4-7cfbe8c07fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "jbhBrqM7jXbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47e98cf-0f5c-45ec-8b6d-7f6ef52fd933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read CSV files"
      ],
      "metadata": {
        "id": "S4rM3-yDjsMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('dummy.csv', newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        pass"
      ],
      "metadata": {
        "id": "A9Kg_xDIjuRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Json files"
      ],
      "metadata": {
        "id": "7tUAOTFmknlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('dummy.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "jcvjan_Uj1-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pickle\n",
        "\n",
        "The pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy."
      ],
      "metadata": {
        "id": "UKQPOkZDqqbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_object = {\"hey\": [1], \"hi\": [46]}\n",
        "import pickle \n",
        "with open(\"dummy.pickle\", 'wb') as f:\n",
        "    pickle.dump(dummy_object, f)\n",
        "with open(\"dummy.pickle\", \"rb\") as f:\n",
        "    d = pickle.load(f)\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrmWVis0rI6B",
        "outputId": "36300c5b-9de9-4df5-aecb-57449338d7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hey': [1], 'hi': [46]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numpy\n",
        "NumPy gives you an enormous range of fast and efficient ways of creating arrays and manipulating numerical data inside them. While a Python list can contain different data types within a single list, all of the elements in a NumPy array should be homogeneous. The mathematical operations that are meant to be performed on arrays would be extremely inefficient if the arrays weren’t homogeneous.\n",
        "\n",
        "NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use. NumPy uses much less memory to store data and it provides a mechanism of specifying the data types. This allows the code to be optimized even further."
      ],
      "metadata": {
        "id": "zCEQALrkkxxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://transfer.sh/kT27Qo/20230228_160723.jpg\" alt=\"Scalar, Vectors and Tensors\" style=\"height: 100px; width:100px;\"/>"
      ],
      "metadata": {
        "id": "22nx400E6CWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([\n",
        "              [1, 2], \n",
        "              [3, 4]\n",
        "              ])\n",
        "y = np.array([[5, 6]])\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Pkn1c2mKo4",
        "outputId": "6a6ce2eb-60ee-4209-c470-d131a12b6292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 2), (1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.dot(y, x)\n",
        "z, z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sttexKnUnA0A",
        "outputId": "abbbbc7f-a8e8-49d6-d496-648f4ce6b282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[23, 34]]), (1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.concatenate([x, y], axis=0)\n",
        "z, z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNdQDSOipamO",
        "outputId": "a5ff56f8-fb45-41db-8058-5524589f938b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]]), (3, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[[[[[[[1, 2, 3]]]]]]]])\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UVyfctZbYky",
        "outputId": "7b00f0c6-08f6-4c07-c72f-1c7941ce70f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 1, 1, 1, 1, 1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.squeeze(a), np.squeeze(a).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUWjVneQbg03",
        "outputId": "ae66dcfb-2e4b-4003-f104-e245113101ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 2, 3]), (3,))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alright, Hands on Keyboard!!\n",
        "\n",
        "Okay, let's start coding a simple regression to predict whether a given word is verb or not. You will use GloVe word embedding to extract the word meaning, then you will input it to a simple regerssion and output the probability of being a verb! In other words, you will calculate : \\\\\n",
        "\n",
        "<h1><center>P(W is verb | W=\"sample\")</center></h1>\n",
        "\n",
        "Please run these cells to gather required dataset. "
      ],
      "metadata": {
        "id": "PNDTofPY1RSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words are ready! You have to download GloVe embedding vectors from this [link](http://nlp.stanford.edu/data/glove.6B.zip) using \"wget\" command"
      ],
      "metadata": {
        "id": "yDBkUrh_Bd7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## YOUR CODE HERE #############\n",
        "!wget 'http://nlp.stanford.edu/data/glove.6B.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkbHew8wBceS",
        "outputId": "fec0ef1d-9b69-4f15-8a33-e0965f54fe22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-04 07:26:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-03-04 07:26:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-03-04 07:26:26--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  4.77MB/s    in 2m 43s  \n",
            "\n",
            "2023-03-04 07:29:10 (5.05 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip downloaded file! cmd: unzip\n",
        "###### YOUR CODE HERE ###########\n",
        "!unzip 'glove.6B.zip.1'"
      ],
      "metadata": {
        "id": "xtPCS0t7D8UX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a71794-67ee-483a-e69a-420335f6cc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip.1\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is loading glove embeddings into a map! You have a mapping from a \"Word\" to is embedding!"
      ],
      "metadata": {
        "id": "si-vHM5DCE7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    coefs = np.expand_dims(coefs, axis=0)\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "MWdtF2rbEZPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index['seek'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kznhIiiSEokq",
        "outputId": "9206534d-5bf4-44f6-9778-348e95dde59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our embeddings are ready lets gather some english words! Run cells below!!"
      ],
      "metadata": {
        "id": "IF8aah5ZGcHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC3r024Q-C1L",
        "outputId": "9f47e05a-de9d-4a8c-f70b-9d1128fd8264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import random\n",
        "SEED = 4678\n",
        "VERBS = []\n",
        "for synset in list(wn.all_synsets(wn.VERB)):\n",
        "    verb = synset.name().split('.')[0]\n",
        "    if(verb in embeddings_index):\n",
        "        VERBS.append(verb)\n",
        "\n",
        "random.Random(SEED).shuffle(VERBS)\n",
        "NOUNS = []\n",
        "\n",
        "for synset in list(wn.all_synsets(wn.NOUN)):\n",
        "    noun = synset.name().split('.')[0]\n",
        "    if(noun in embeddings_index):\n",
        "        NOUNS.append(noun)\n",
        "\n",
        "\n",
        "random.Random(SEED).shuffle(NOUNS)\n"
      ],
      "metadata": {
        "id": "CjYk83_-ptMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(VERBS), len(NOUNS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1A2fBX1_aBZ",
        "outputId": "c2afe75c-b1e7-4c43-f326-a1e46f47e872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10891, 43680)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOUNS = NOUNS[:100]\n",
        "VERBS = VERBS[:100]\n",
        "\n",
        "x_train = NOUNS[:64] + VERBS[:64]\n",
        "y_train = [0] * 64 + [1] * 64\n",
        "x_test = NOUNS[64:] + VERBS[64:]\n",
        "y_test = [0] * 36 + [1] * 36\n",
        "\n",
        "c = list(zip(x_train, y_train))\n",
        "random.Random(SEED).shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "y_train = np.array(y_train)\n",
        "c = list(zip(x_test, y_test))\n",
        "random.Random(SEED).shuffle(c)\n",
        "x_test, y_test = zip(*c)\n",
        "y_test = np.array(y_test)\n"
      ],
      "metadata": {
        "id": "AL_bynbXAx6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in x_train + x_test : \n",
        "    assert len(embeddings_index[word][0]) == 100"
      ],
      "metadata": {
        "id": "fLw6PUHUEwQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is ready, time to implement our Logistic regression model! Logistic regression takes a vector as input and outputs a probability for each vector!! This model uses Sigmoid function:\n",
        "\n",
        "\n",
        "The exact formula of sigmoid is:\n",
        "<h1><center>$σ(x) = \\frac{1}{1 + e^{-x}}$</center></h1>\n",
        " "
      ],
      "metadata": {
        "id": "-Mp4diFBHVx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn.hackaday.io/images/5998011530593785298.png\" alt=\"Scalar, Vectors and Tensors\" style=\"height: 100px; width:100px;\"/>"
      ],
      "metadata": {
        "id": "8eUiDduaKfNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis should be like:\n",
        "\n",
        "<h1><canter>$H(X, w, b) = Sigmoid(W^T.X + b)$</center></h1>\n",
        "\n",
        "Cost function should be like:\n",
        "\n",
        "<h1><canter>$J = -\\Sigma_mYlog(H) + (1 - Y)log(1-H)$</center></h1>"
      ],
      "metadata": {
        "id": "aYUWHH3eZd5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.01, num_iterations = 2000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.w = []\n",
        "        self.b = 0\n",
        "    \n",
        "    def initialize_weight(self,dim):\n",
        "        \"\"\"\n",
        "        This function creates a vector of zeros of shape (dim, 1)  for w and initializes b to 0.\n",
        "        returns w and b\n",
        "        Argument:\n",
        "        dim -- size of the w vector we want (or number of parameters  in this case)\n",
        "        \"\"\"\n",
        "        #### YOUR CODE HERE #####\n",
        "        w = np.zeros(dim)\n",
        "        b = 0\n",
        "        return w, b\n",
        "    \n",
        "    def sigmoid(self,z):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid of z\n",
        "        Argument:\n",
        "        z -- is the decision boundary of the classifier\n",
        "        \"\"\"\n",
        "        #### YOUR CODE HERE #####\n",
        "        s = 1 / (1 + np.exp(-z))\n",
        "        return s\n",
        "\n",
        "    def hypothesis(self,w,X,b):\n",
        "        \"\"\"\n",
        "        This function calculates the hypothesis for the present model\n",
        "        Argument:\n",
        "        w -- weight vector\n",
        "        X -- The input vector\n",
        "        b -- The bias vector\n",
        "        \"\"\"\n",
        "        #### YOUR CODE HERE #####\n",
        "        h = self.sigmoid(np.dot(w.T, X) + b)\n",
        "        return h\n",
        "    \n",
        "    def cost(self,H,Y,m):\n",
        "        \"\"\"\n",
        "        This function calculates the cost of hypothesis\n",
        "        Arguments: \n",
        "        H -- The hypothesis vector \n",
        "        Y -- The output \n",
        "        m -- Number training samples\n",
        "        \"\"\"\n",
        "        #### YOUR CODE HERE #####\n",
        "        c = -np.sum((Y * np.log(H)) + ((1 - Y) * np.log(1 - H))) / m\n",
        "        return np.squeeze(c)\n",
        "    \n",
        "    def cal_gradient(self, w,H,X,Y):\n",
        "        \"\"\"\n",
        "        Calculates gradient of the given model in learning space\n",
        "        \"\"\"\n",
        "        m = X.shape[1]\n",
        "        dw = np.dot(X,(H-Y).T)/m\n",
        "        db = np.sum(H-Y)/m\n",
        "        grads = {\"dw\": dw,\n",
        "                 \"db\": db}\n",
        "        return grads\n",
        " \n",
        "    def predict(self,X):\n",
        "        '''\n",
        "        Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "        Arguments:\n",
        "        w -- weights, a numpy array of size (n, 1)\n",
        "        b -- bias, a scalar\n",
        "        X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "        Returns:\n",
        "        Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "        '''\n",
        "        X = np.array(X)\n",
        "        m = X.shape[1]\n",
        "  \n",
        "        Y_prediction = np.zeros((1,m))\n",
        "  \n",
        "        w = self.w.reshape(X.shape[0], 1)\n",
        "        b = self.b\n",
        "        # Compute vector \"H\" \n",
        "        H = self.hypothesis(w, X, b)\n",
        " \n",
        "        for i in range(H.shape[1]):\n",
        "        # Convert probabilities H[0,i] to actual predictions p[0,i]\n",
        "            if H[0,i] >= 0.5:\n",
        "                Y_prediction[0,i] = 1\n",
        "            else: \n",
        "                Y_prediction[0,i] = 0\n",
        "   \n",
        "        return Y_prediction\n",
        "    def gradient_position(self, w, b, X, Y):\n",
        "        \"\"\"\n",
        "        It just gets calls various functions to get status of learning model\n",
        "        Arguments:\n",
        "        w -- weights, a numpy array of size (dim, 1)\n",
        "        b -- bias, a scalar\n",
        "        X -- data of size (b, dim)\n",
        "        Y -- true \"label\" vector (containing 0 or 1 ) of size (b, number of examples)\n",
        "        \"\"\"\n",
        "        m = X.shape[1]\n",
        "        H = self.hypothesis(w,X,b)         # compute activation\n",
        "        cost = self.cost(H,Y,m)               # compute cost\n",
        "        grads = self.cal_gradient(w, H, X, Y) # compute gradient\n",
        "        return grads, cost\n",
        "    \n",
        "    def gradient_descent(self, w, b, X, Y, print_cost = False):\n",
        "        \"\"\"\n",
        "        This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "        Arguments:\n",
        "        w — weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "        b — bias, a scalar\n",
        "        X -- data of size (no. of features, number of examples)\n",
        "        Y -- true \"label\" vector (containing 0 or 1 ) of size (1, number of examples)\n",
        "        print_cost — True to print the loss every 100 steps\n",
        "    \n",
        "        Returns:\n",
        "        params — dictionary containing the weights w and bias b\n",
        "        grads — dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "        costs — list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "        \"\"\"\n",
        " \n",
        "        costs = []\n",
        " \n",
        "        for i in range(self.num_iterations):\n",
        "        # Cost and gradient calculation \n",
        "            grads, cost = self.gradient_position(w,b,X,Y)\n",
        " \n",
        " \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        " \n",
        "        # update rule \n",
        "        w = w - (self.learning_rate * dw) \n",
        "        b = b - (self.learning_rate * db)\n",
        " \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        " \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        " \n",
        " \n",
        "        params = {\"w\": w,\n",
        "                  \"b\": b}\n",
        " \n",
        "        grads = {\n",
        "                 \"dw\": dw,\n",
        "                 \"db\": db\n",
        "                 }\n",
        "    \n",
        "        return params, grads, costs\n",
        "    \n",
        "    def train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):\n",
        "        \"\"\"\n",
        "        Builds the logistic regression model by calling the function you’ve implemented previously\n",
        "    \n",
        "        Arguments:\n",
        "        X_train — training set represented by a numpy array of shape (features, m_train)\n",
        "        Y_train — training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "        X_test — test set represented by a numpy array of shape (features, m_test)\n",
        "        Y_test — test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "        print_cost — Set to true to print the cost every 100 iterations\n",
        "    \n",
        "        Returns:\n",
        "        d — dictionary containing information about the model.\n",
        "        \"\"\"\n",
        "        # initialize parameters with zeros \n",
        "        dim = np.shape(X_train)[0]\n",
        "        w, b = self.initialize_weight(dim)\n",
        "        # Gradient descent \n",
        "        \n",
        "        parameters, grads, costs = self.gradient_descent(w, b, X_train, Y_train, print_cost = False)\n",
        " \n",
        "        # Retrieve parameters w and b from dictionary “parameters”\n",
        "        self.w = parameters[\"w\"]\n",
        "        self.b = parameters[\"b\"]\n",
        " \n",
        "        # Predict test/train set examples \n",
        "        Y_prediction_test = self.predict(X_test)\n",
        "        Y_prediction_train = self.predict(X_train)\n",
        "        # Print train/test Errors\n",
        "        train_score = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
        "        test_score = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
        "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "        d = {\"costs\": costs,\n",
        "             \"w\" : self.w, \n",
        "             \"b\" : self.b,\n",
        "             \"learning_rate\": self.learning_rate,\n",
        "             \"num_iterations\": self.num_iterations,\n",
        "             \"train accuracy\": train_score,\n",
        "             \"test accuracy\" : test_score}\n",
        " \n",
        "        return d"
      ],
      "metadata": {
        "id": "wUt6ZDLeHUWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats!! We've got a simple regression model, time to concatenate all vector embeddings and train the model on it!!"
      ],
      "metadata": {
        "id": "XiM9dxwdo5zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "X_test = []\n",
        "for word in x_train:\n",
        "    ###### YOUR CODE HERE #####\n",
        "    # you have to add each word embedding to train array\n",
        "    X_train.append(embeddings_index[word])\n",
        "\n",
        "for word in x_test:\n",
        "    ###### YOUR CODE HERE #####\n",
        "    # you have to add each word embedding to test array\n",
        "    X_test.append(embeddings_index[word])\n",
        "\n",
        "X_train = np.concatenate(X_train, axis = 0) # Conacat all embedding to a simple matrix\n",
        "X_test =  np.concatenate(X_test, axis = 0) # Concat all embeddings to a simple matrix\n",
        "\n",
        "X_train = X_train.transpose(1, 0)\n",
        "X_test = X_test.transpose(1, 0)"
      ],
      "metadata": {
        "id": "isDqThJsLVn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape\n",
        "\n",
        "#exptected: ((300, 128), (300, 72))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmzMRMavhcZS",
        "outputId": "7a0e58e5-b58c-497a-941a-37051f270146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 128), (100, 72))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results!"
      ],
      "metadata": {
        "id": "FGxZk3-Dp1eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression = LogisticRegression()\n",
        "results = regression.train_model(X_train, y_train, X_test, y_test, print_cost = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbnZQ_wqjmu2",
        "outputId": "c3ac8c11-18f4-45af-f1c5-874bb4ca5d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy: 77.34375 %\n",
            "test accuracy: 70.83333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save them!\n",
        "\n",
        "Save your results in a pickle and move it to your google drive!"
      ],
      "metadata": {
        "id": "amvA0I3_qCGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### YOUR CODE HERE #####\n",
        "import pickle\n",
        "with open(\"/content/drive/My Drive/results_100.pickle\", 'wb') as f:\n",
        "  pickle.dump(results, f)"
      ],
      "metadata": {
        "id": "ueEs2E6EnKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HAVE FUN"
      ],
      "metadata": {
        "id": "ek74DsAKq2kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reason\n",
        "<font color=\"orange\">Dimensionaly reduction may lead to some amount of data loss. therefore, Accuracy is compromised.</font>"
      ],
      "metadata": {
        "id": "O17H7qnCAX-X"
      }
    }
  ]
}